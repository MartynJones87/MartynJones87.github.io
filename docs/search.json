[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Martyn. I am currently a Computer Science Technical Lead in the Repair Analytics and Data department at J.D. Power.\nMy role covers a variety of different areas:\n\nSQL Server database administration and development\nEverything from configuring new instances, writing and managing multi billion row ETL processes and writing stored procedures to perform complex data analysis. Most of the time I’m trying convince SQL Server to do something it wasn’t really designed to do, in as fast a way as possible!\n\n\nC# application and web-application development\nOne day, this could be writing front-end TypeScript code for a ASP.Net Web application and the next, coding high performance command line applications to process billion row CSV files or handling hundreds of millions of data files.\n\n\nSystems and Network Administration\nI’m part of the core team that manages the network and servers, so turn my hand to whatever situations arise.\nI live just outside of Swansea in Wales, UK with my wife and son and our dog, Bob and cat, Jasper."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Selecting Random Samples From Large SQL Server Tables\n\n\n\n\n\n\n\nsql-server\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nMartyn Jones\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/selecting-random-samples-from-large-sql-server-tables/index.html",
    "href": "posts/selecting-random-samples-from-large-sql-server-tables/index.html",
    "title": "Selecting Random Samples From Large SQL Server Tables",
    "section": "",
    "text": "Recently, we needed to select 10,000 random rows from a table containing a billion rows.\nI couldn’t think of a better solution than ordering by NEWID() and selecting the TOP (10000) rows, but given the size of the table we’re working with, it was very slow and inefficient, so there had to be a better solution.\nThankfully, Brent Ozar had a blog post to get us moving in the right direction: How to Get a Random Row from a Large Table.\nAs Brent’s post explains, the ordering by NEWID() is so slow because it calculates the NEWID value for every row in the table and then filters the return to the first 10,000 rows.\nUltimately, the best solution from Brent’s post involves:\n\nGetting the largest ID value from the table.\nGenerating a random number.\nGetting the modulo of that random number for the maximum ID value in the table.\nSelecting the record with that ID value.\n\nThis does require a table with an integer ID column and also, is not guaranteed to return a record, if the randomly selected ID value doesn’t exist in the table.\nFor our purposes though, we had a BIGINT identity column and the very vast majority of ID values between 1 and the max value did exist, so this was perfect.\nThere is also a nod to our required use in Brent’s post:\n\nIf you wanted 10 rows, you’d have to call code like this 10 times (or generate 10 random numbers and use an IN clause.)\n\nAs we wanted 10,000 rows, we decided to: - Use a Tally table, (called dbo.Numbers in this example) to get the 10,000 rows. - Generate a NEWID() for each of those rows. - Store those 10,000 random numbers in a temporary table.\n/* Get the highest ID in the table to use in the modulo calculation. */\nDECLARE @MaxID BIGINT;\nSELECT @MaxID = MAX(ID)\nFROM dbo.TableToSample;\n\n/* Create a table variable to store the randomly selected IDs. */\nDECLARE @RandomIDs TABLE (\n    SampleID INT NOT NULL,\n    RandomID BIGINT NOT NULL,\n    PRIMARY KEY CLUSTERED (RandomID, SampleID)\n);\n\n/* To allow for any IDs no longer present in the table, select 10% more than needed at this point. */\nINSERT INTO @RandomIDs (RandomID)\nSELECT  n.N AS SampleID,\n        ABS(CHECKSUM(NEWID())) % MaxID AS RandomID\nFROM dbo.Numbers AS n\nWHERE n.N BETWEEN 1 AND 11000;\n\n/* Select from the table to be sampled inner joined with the temporary table to only return rows \n * with a randomly selected ID value, and only keep the first 10,000 matches. */\nSELECT TOP (10000)\n    tts.ID, \n    tts.Col1, \n    tts.Col2\nFROM dbo.TableToSample AS tts \nINNER JOIN @RandomIDs AS rnd\n    ON tts.ID = rnd.RandomID\nORDER BY rnd.SampleID;\nUltimately, the performance of this approach is significantly faster than the alternative and requires far less IO and CPU time. It solved the problem and is something that has proved useful on a few occasions since."
  }
]